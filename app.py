import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from io import BytesIO
import plotly.express as px

# T·∫£i d·ªØ li·ªáu m·∫∑c ƒë·ªãnh t·ª´ GitHub (file2, file3, file4)
@st.cache_data
def load_default_data():
    url_file2 = "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file2.xlsx"
    url_file3 = "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/file3.xlsx"
    url_file4 = "https://raw.githubusercontent.com/phamtai1211/Thau_3PPharma/main/nhom_dieu_tri.xlsx"

    file2 = pd.read_excel(BytesIO(requests.get(url_file2).content))
    file3 = pd.read_excel(BytesIO(requests.get(url_file3).content))
    file4 = pd.read_excel(BytesIO(requests.get(url_file4).content))
    return file2, file3, file4

file2, file3, file4 = load_default_data()

# H√†m ti·ªán √≠ch chu·∫©n h√≥a chu·ªói ƒë·ªÉ so s√°nh (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng, kho·∫£ng tr·∫Øng, k√Ω t·ª± ƒë·∫∑c bi·ªát)
def normalize_active(name: str) -> str:
    # B·ªè n·ªôi dung trong ngo·∫∑c ƒë∆°n, chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng, b·ªè d∆∞ kho·∫£ng tr·∫Øng
    return re.sub(r'\s+', ' ', re.sub(r'\(.*?\)', '', str(name))).strip().lower()

def normalize_concentration(conc: str) -> str:
    s = str(conc).lower()
    # Thay d·∫•u ph·∫©y b·∫±ng d·∫•u ch·∫•m (cho s·ªë th·∫≠p ph√¢n)
    s = s.replace(',', '.')
    # B·ªè c·ª•m 'dung t√≠ch'
    s = s.replace('dung t√≠ch', '')
    # T√°ch c√°c ph·∫ßn b·ªüi d·∫•u comma n·∫øu c√≥
    parts = [p.strip() for p in s.split(',') if p.strip() != '']
    # Lo·∫°i b·ªè c√°c ph·∫ßn ch·ªâ ch·ª©a ch·ªØ (m√¥ t·∫£) kh√¥ng c√≥ s·ªë
    parts = [p for p in parts if re.search(r'\d', p)]
    # N·∫øu c√≥ 2 ph·∫ßn d·∫°ng "X mg" v√† "Y ml" th√¨ gh√©p th√†nh "Xmg/Yml"
    if len(parts) >= 2 and re.search(r'(mg|mcg|g|%)', parts[0]) and 'ml' in parts[-1] and '/' not in parts[0]:
        conc_norm = parts[0].replace(' ', '') + '/' + parts[-1].replace(' ', '')
    else:
        conc_norm = ''.join([p.replace(' ', '') for p in parts])
    # Chu·∫©n h√≥a d·∫•u c·ªông (n·∫øu c√≥ d·∫°ng "mg + mg")
    conc_norm = conc_norm.replace('+', '+')
    return conc_norm

def normalize_group(grp: str) -> str:
    # Tr√≠ch ph·∫ßn s·ªë trong m√£ nh√≥m thu·ªëc (vd "Nh√≥m 4" -> "4", "N4" -> "4")
    return re.sub(r'\D', '', str(grp)).strip()

# Sidebar: Ch·ªçn ch·ª©c nƒÉng ch√≠nh
st.sidebar.title("Ch·ª©c nƒÉng")
option = st.sidebar.radio("Ch·ªçn ch·ª©c nƒÉng", 
    ["L·ªçc Danh M·ª•c Th·∫ßu", "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu", "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu", "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai"])

# 1. L·ªçc Danh M·ª•c Th·∫ßu
if option == "L·ªçc Danh M·ª•c Th·∫ßu":
    st.header("üìÇ L·ªçc Danh M·ª•c Th·∫ßu")
    # Ch·ªçn Mi·ªÅn
    regions = sorted(file3["Mi·ªÅn"].dropna().unique())
    selected_region = st.selectbox("Ch·ªçn Mi·ªÅn", regions)
    sub_df = file3[file3["Mi·ªÅn"] == selected_region] if selected_region else file3.copy()
    # Ch·ªçn V√πng (n·∫øu c√≥)
    areas = sorted(sub_df["V√πng"].dropna().unique())
    selected_area = None
    if areas:
        selected_area = st.selectbox("Ch·ªçn V√πng", ["(T·∫•t c·∫£)"] + areas)
        if selected_area and selected_area != "(T·∫•t c·∫£)":
            sub_df = sub_df[sub_df["V√πng"] == selected_area]
    # Ch·ªçn T·ªânh
    provinces = sorted(sub_df["T·ªânh"].dropna().unique())
    selected_prov = st.selectbox("Ch·ªçn T·ªânh", provinces)
    sub_df = sub_df[sub_df["T·ªânh"] == selected_prov] if selected_prov else sub_df
    # Ch·ªçn B·ªánh vi·ªán/SYT
    hospitals = sorted(sub_df["B·ªánh vi·ªán/SYT"].dropna().unique())
    selected_hospital = st.selectbox("Ch·ªçn B·ªánh vi·ªán/S·ªü Y T·∫ø", hospitals)
    # Upload file danh m·ª•c m·ªùi th·∫ßu
    uploaded_file = st.file_uploader("T·∫£i l√™n file Danh M·ª•c M·ªùi Th·∫ßu (.xlsx)", type=["xlsx"])
    if uploaded_file is not None and selected_hospital:
        # ƒê·ªçc Excel v√† x√°c ƒë·ªãnh sheet ch·ª©a d·ªØ li·ªáu ch√≠nh
        xls = pd.ExcelFile(uploaded_file)
        sheet_name = None
        max_cols = 0
        for name in xls.sheet_names:
            try:
                df_test = xls.parse(name, nrows=1, header=None)
                cols = df_test.shape[1]
            except Exception:
                cols = 0
            if cols > max_cols:
                max_cols = cols
                sheet_name = name
        if sheet_name is None:
            st.error("‚ùå Kh√¥ng t√¨m th·∫•y sheet d·ªØ li·ªáu ph√π h·ª£p trong file.")
        else:
            # ƒê·ªçc to√†n b·ªô sheet (kh√¥ng ƒë·∫∑t header) ƒë·ªÉ t√¨m d√≤ng ti√™u ƒë·ªÅ
            df_raw = pd.read_excel(uploaded_file, sheet_name=sheet_name, header=None)
            header_index = None
            for i in range(10):
                row = " ".join(df_raw.iloc[i].fillna('').astype(str).tolist())
                if "T√™n ho·∫°t ch·∫•t" in row and "S·ªë l∆∞·ª£ng" in row:
                    header_index = i
                    break
            if header_index is None:
                st.error("‚ùå Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c d√≤ng ti√™u ƒë·ªÅ trong file.")
            else:
                # T·∫°o DataFrame v·ªõi header ch√≠nh x√°c
                header = df_raw.iloc[header_index].tolist()
                df_all = df_raw.iloc[header_index+1:].reset_index(drop=True)
                df_all.columns = header
                # B·ªè c√°c d√≤ng tr·ªëng ho√†n to√†n (n·∫øu c√≥)
                df_all = df_all.dropna(how='all').reset_index(drop=True)
                # So s√°nh 3 c·ªôt (ho·∫°t ch·∫•t, h√†m l∆∞·ª£ng, nh√≥m thu·ªëc) v·ªõi danh m·ª•c c√¥ng ty (file2)
                df_all["active_norm"] = df_all["T√™n ho·∫°t ch·∫•t"].apply(normalize_active)
                df_all["conc_norm"] = df_all["N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng"].apply(normalize_concentration)
                df_all["group_norm"] = df_all["Nh√≥m thu·ªëc"].apply(normalize_group)
                df_comp = file2.copy()
                df_comp["active_norm"] = df_comp["T√™n ho·∫°t ch·∫•t"].apply(normalize_active)
                df_comp["conc_norm"] = df_comp["N·ªìng ƒë·ªô/H√†m l∆∞·ª£ng"].apply(normalize_concentration)
                df_comp["group_norm"] = df_comp["Nh√≥m thu·ªëc"].apply(normalize_group)
                # Inner merge ƒë·ªÉ gi·ªØ l·∫°i c√°c d√≤ng kh·ªõp v·ªõi danh m·ª•c c√¥ng ty
                merged_df = pd.merge(df_all, df_comp, on=["active_norm", "conc_norm", "group_norm"], how="inner", suffixes=(None, "_comp"))
                # Ch·ªçn c√°c c·ªôt g·ªëc + t√™n s·∫£n ph·∫©m (brand), ƒë·ªìng th·ªùi g·∫Øn ƒê·ªãa b√†n v√† Kh√°ch h√†ng ph·ª• tr√°ch
                result_columns = df_all.columns.tolist() + ["T√™n s·∫£n ph·∫©m"]
                result_df = merged_df[result_columns].copy()
                # Th√™m th√¥ng tin ƒê·ªãa b√†n, Kh√°ch h√†ng ph·ª• tr√°ch t·ª´ file3
                hosp_data = file3[file3["B·ªánh vi·ªán/SYT"] == selected_hospital][["T√™n s·∫£n ph·∫©m", "ƒê·ªãa b√†n", "T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai"]]
                result_df = pd.merge(result_df, hosp_data, on="T√™n s·∫£n ph·∫©m", how="left")
                # T√≠nh c·ªôt "T·ª∑ tr·ªçng SL/DM T·ªïng"
                # L·∫≠p b·∫£ng t·ªïng s·ªë l∆∞·ª£ng theo Nh√≥m ƒëi·ªÅu tr·ªã cho to√†n b·ªô danh m·ª•c th·∫ßu (df_all)
                # √Ånh x·∫° ho·∫°t ch·∫•t -> nh√≥m ƒëi·ªÅu tr·ªã t·ª´ file4
                treat_map = { normalize_active(a): grp for a, grp in zip(file4["Ho·∫°t ch·∫•t"], file4["Nh√≥m ƒëi·ªÅu tr·ªã"]) }
                group_total = {}
                for _, row in df_all.iterrows():
                    act = normalize_active(row["T√™n ho·∫°t ch·∫•t"])
                    group = treat_map.get(act)
                    qty = pd.to_numeric(row.get("S·ªë l∆∞·ª£ng", 0), errors='coerce')
                    if pd.isna(qty):
                        qty = 0
                    if group:
                        group_total[group] = group_total.get(group, 0) + float(qty)
                # T√≠nh t·ª∑ tr·ªçng cho t·ª´ng d√≤ng k·∫øt qu·∫£
                ratios = []
                for _, row in result_df.iterrows():
                    act = normalize_active(row["T√™n ho·∫°t ch·∫•t"])
                    group = treat_map.get(act)
                    qty = pd.to_numeric(row.get("S·ªë l∆∞·ª£ng", 0), errors='coerce')
                    if pd.isna(qty) or group is None or group not in group_total or group_total[group] == 0:
                        ratios.append(None)
                    else:
                        ratio = float(qty) / group_total[group]
                        ratios.append(f"{ratio:.2%}")
                result_df["T·ª∑ tr·ªçng SL/DM T·ªïng"] = ratios
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ l·ªçc v√† n√∫t t·∫£i v·ªÅ
                st.success(f"‚úÖ ƒê√£ l·ªçc ƒë∆∞·ª£c {len(result_df)} m·ª•c thu·ªëc thu·ªôc danh m·ª•c c√¥ng ty.")
                st.dataframe(result_df.head(10))
                # Xu·∫•t file Excel k·∫øt qu·∫£
                output = BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    result_df.to_excel(writer, sheet_name="KetQuaLoc", index=False)
                st.download_button("‚¨áÔ∏è T·∫£i File K·∫øt Qu·∫£", data=output.getvalue(), file_name="Ketqua_loc.xlsx")
                # L∆∞u DataFrame ƒë√£ l·ªçc v√†o session_state ƒë·ªÉ d√πng cho ph√¢n t√≠ch
                st.session_state["filtered_df"] = result_df
                st.session_state["selected_hospital"] = selected_hospital

# 2. Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu
elif option == "Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu":
    st.header("üìä Ph√¢n T√≠ch Danh M·ª•c Th·∫ßu")
    if "filtered_df" not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán b∆∞·ªõc 'L·ªçc Danh M·ª•c Th·∫ßu' tr∆∞·ªõc.")
    else:
        df_filtered = st.session_state["filtered_df"].copy()
        # ƒê·∫£m b·∫£o ki·ªÉu d·ªØ li·ªáu s·ªë
        df_filtered["S·ªë l∆∞·ª£ng"] = pd.to_numeric(df_filtered["S·ªë l∆∞·ª£ng"], errors='coerce').fillna(0)
        df_filtered["Gi√° k·∫ø ho·∫°ch"] = pd.to_numeric(df_filtered["Gi√° k·∫ø ho·∫°ch"], errors='coerce').fillna(0)
        # Th√™m c·ªôt tr·ªã gi√° = S·ªë l∆∞·ª£ng * Gi√° k·∫ø ho·∫°ch
        df_filtered["Tr·ªã gi√°"] = df_filtered["S·ªë l∆∞·ª£ng"] * df_filtered["Gi√° k·∫ø ho·∫°ch"]
        # Bi·ªÉu ƒë·ªì 1: Nh√≥m th·∫ßu s·ª≠ d·ª•ng nhi·ªÅu nh·∫•t theo tr·ªã gi√°
        group_val = df_filtered.groupby("Nh√≥m thu·ªëc")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False)
        fig1 = px.bar(group_val, x="Nh√≥m thu·ªëc", y="Tr·ªã gi√°", title="Tr·ªã gi√° theo Nh√≥m th·∫ßu (g√≥i th·∫ßu)")
        st.plotly_chart(fig1, use_container_width=True)
        # Bi·ªÉu ƒë·ªì 2: Ph√¢n t√≠ch ƒë∆∞·ªùng d√πng (ti√™m/u·ªëng) theo tr·ªã gi√°
        # X√°c ƒë·ªãnh lo·∫°i ƒë∆∞·ªùng d√πng cho t·ª´ng m·ª•c (Ti√™m, U·ªëng ho·∫∑c Kh√°c)
        route_df = df_filtered.copy()
        def classify_route(route_str):
            route = str(route_str).lower()
            if "ti√™m" in route:
                return "Ti√™m"
            elif "u·ªëng" in route:
                return "U·ªëng"
            else:
                return "Kh√°c"
        route_df["Lo·∫°i ƒë∆∞·ªùng d√πng"] = route_df["ƒê∆∞·ªùng d√πng"].apply(classify_route)
        route_val = route_df.groupby("Lo·∫°i ƒë∆∞·ªùng d√πng")["Tr·ªã gi√°"].sum().reset_index()
        fig2 = px.pie(route_val, names="Lo·∫°i ƒë∆∞·ªùng d√πng", values="Tr·ªã gi√°", title="T·ª∑ tr·ªçng tr·ªã gi√° theo ƒë∆∞·ªùng d√πng")
        st.plotly_chart(fig2, use_container_width=True)
        # Bi·ªÉu ƒë·ªì 3: Top 10 ho·∫°t ch·∫•t theo S·ªë l∆∞·ª£ng
        top_active_qty = df_filtered.groupby("T√™n ho·∫°t ch·∫•t")["S·ªë l∆∞·ª£ng"].sum().reset_index().sort_values("S·ªë l∆∞·ª£ng", ascending=False).head(10)
        fig3 = px.bar(top_active_qty, x="T√™n ho·∫°t ch·∫•t", y="S·ªë l∆∞·ª£ng", title="Top 10 Ho·∫°t ch·∫•t (theo S·ªë l∆∞·ª£ng)")
        st.plotly_chart(fig3, use_container_width=True)
        # Bi·ªÉu ƒë·ªì 4: Top 10 ho·∫°t ch·∫•t theo Tr·ªã gi√°
        top_active_val = df_filtered.groupby("T√™n ho·∫°t ch·∫•t")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False).head(10)
        fig4 = px.bar(top_active_val, x="T√™n ho·∫°t ch·∫•t", y="Tr·ªã gi√°", title="Top 10 Ho·∫°t ch·∫•t (theo Tr·ªã gi√°)")
        st.plotly_chart(fig4, use_container_width=True)
        # Bi·ªÉu ƒë·ªì 5: Ph√¢n t√≠ch Nh√≥m ƒëi·ªÅu tr·ªã v√† top 10 s·∫£n ph·∫©m
        # G·∫Øn c·ªôt Nh√≥m ƒëi·ªÅu tr·ªã cho t·ª´ng m·ª•c
        treat_map = { normalize_active(a): grp for a, grp in zip(file4["Ho·∫°t ch·∫•t"], file4["Nh√≥m ƒëi·ªÅu tr·ªã"]) }
        df_filtered["Nh√≥m ƒëi·ªÅu tr·ªã"] = df_filtered["T√™n ho·∫°t ch·∫•t"].apply(lambda x: treat_map.get(normalize_active(x), "Kh√°c"))
        # T·ªïng tr·ªã gi√° theo nh√≥m ƒëi·ªÅu tr·ªã
        treat_val = df_filtered.groupby("Nh√≥m ƒëi·ªÅu tr·ªã")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False)
        fig5 = px.bar(treat_val, x="Tr·ªã gi√°", y="Nh√≥m ƒëi·ªÅu tr·ªã", orientation='h', title="Tr·ªã gi√° theo Nh√≥m ƒëi·ªÅu tr·ªã")
        st.plotly_chart(fig5, use_container_width=True)
        # Ch·ªçn nh√≥m ƒëi·ªÅu tr·ªã ƒë·ªÉ xem Top 10 s·∫£n ph·∫©m
        groups = treat_val["Nh√≥m ƒëi·ªÅu tr·ªã"].tolist()
        selected_grp = st.selectbox("Ch·ªçn Nh√≥m ƒëi·ªÅu tr·ªã ƒë·ªÉ xem Top 10 s·∫£n ph·∫©m", groups)
        if selected_grp:
            top_products = df_filtered[df_filtered["Nh√≥m ƒëi·ªÅu tr·ªã"] == selected_grp].groupby("T√™n s·∫£n ph·∫©m")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False).head(10)
            fig6 = px.bar(top_products, x="Tr·ªã gi√°", y="T√™n s·∫£n ph·∫©m", orientation='h', title=f"Top 10 s·∫£n ph·∫©m - Nh√≥m {selected_grp}")
            st.plotly_chart(fig6, use_container_width=True)
        # Bi·ªÉu ƒë·ªì 6: Hi·ªáu qu·∫£ theo T√™n kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai (t·ªïng tr·ªã gi√° theo ng∆∞·ªùi ph·ª• tr√°ch)
        rep_val = df_filtered.groupby("T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False)
        fig7 = px.bar(rep_val, x="Tr·ªã gi√°", y="T√™n Kh√°ch h√†ng ph·ª• tr√°ch tri·ªÉn khai", orientation='h', title="Tr·ªã gi√° theo Kh√°ch h√†ng ph·ª• tr√°ch")
        st.plotly_chart(fig7, use_container_width=True)

# 3. Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu
elif option == "Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu":
    st.header("üèÜ Ph√¢n T√≠ch Danh M·ª•c Tr√∫ng Th·∫ßu")
    win_file = st.file_uploader("T·∫£i l√™n file K·∫øt Qu·∫£ Tr√∫ng Th·∫ßu (.xlsx)", type=["xlsx"])
    invite_file = st.file_uploader("T·∫£i l√™n file Danh M·ª•c M·ªùi Th·∫ßu (ƒë·ªÉ ƒë·ªëi chi·∫øu, t√πy ch·ªçn)", type=["xlsx"])
    if win_file is not None:
        # X√°c ƒë·ªãnh sheet ch√≠nh c·ªßa file tr√∫ng th·∫ßu
        xls_win = pd.ExcelFile(win_file)
        win_sheet = xls_win.sheet_names[0]
        max_cols = 0
        for name in xls_win.sheet_names:
            try:
                df_test = xls_win.parse(name, nrows=1, header=None)
                cols = df_test.shape[1]
            except:
                cols = 0
            if cols > max_cols:
                max_cols = cols
                win_sheet = name
        # ƒê·ªçc to√†n b·ªô sheet v√† x√°c ƒë·ªãnh d√≤ng ti√™u ƒë·ªÅ
        df_win_raw = pd.read_excel(win_file, sheet_name=win_sheet, header=None)
        header_idx = None
        for i in range(10):
            row_text = " ".join(df_win_raw.iloc[i].fillna('').astype(str).tolist())
            if "T√™n ho·∫°t ch·∫•t" in row_text and "Nh√† th·∫ßu tr√∫ng" in row_text:
                header_idx = i
                break
        if header_idx is None:
            st.error("‚ùå Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ti√™u ƒë·ªÅ c·ªôt trong file tr√∫ng th·∫ßu.")
        else:
            header = df_win_raw.iloc[header_idx].tolist()
            df_win = df_win_raw.iloc[header_idx+1:].reset_index(drop=True)
            df_win.columns = header
            df_win = df_win.dropna(how='all').reset_index(drop=True)
            # Chuy·ªÉn ki·ªÉu s·ªë cho S·ªë l∆∞·ª£ng v√† gi√°
            df_win["S·ªë l∆∞·ª£ng"] = pd.to_numeric(df_win.get("S·ªë l∆∞·ª£ng", 0), errors='coerce').fillna(0)
            # X√°c ƒë·ªãnh c·ªôt gi√° tr√∫ng (n·∫øu kh√¥ng c√≥ th√¨ d√πng Gi√° k·∫ø ho·∫°ch)
            price_col = None
            for col in df_win.columns:
                if "Gi√° tr√∫ng" in str(col):
                    price_col = col
                    break
            if price_col is None:
                price_col = "Gi√° k·∫ø ho·∫°ch"
            df_win[price_col] = pd.to_numeric(df_win.get(price_col, 0), errors='coerce').fillna(0)
            # T√≠nh tr·ªã gi√° tr√∫ng th·∫ßu m·ªói m·ª•c
            df_win["Tr·ªã gi√°"] = df_win["S·ªë l∆∞·ª£ng"] * df_win[price_col]
            # Bi·ªÉu ƒë·ªì: Top 20 nh√† th·∫ßu tr√∫ng tr·ªã gi√° cao nh·∫•t
            win_val = df_win.groupby("Nh√† th·∫ßu tr√∫ng")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False).head(20)
            fig_w1 = px.bar(win_val, x="Tr·ªã gi√°", y="Nh√† th·∫ßu tr√∫ng", orientation='h', title="Top 20 Nh√† th·∫ßu tr√∫ng (theo tr·ªã gi√°)")
            st.plotly_chart(fig_w1, use_container_width=True)
            # Bi·ªÉu ƒë·ªì: Ph√¢n t√≠ch theo nh√≥m ƒëi·ªÅu tr·ªã (c∆° c·∫•u tr·ªã gi√°)
            df_win["Nh√≥m ƒëi·ªÅu tr·ªã"] = df_win["T√™n ho·∫°t ch·∫•t"].apply(lambda x: treat_map.get(normalize_active(x), "Kh√°c"))
            treat_win = df_win.groupby("Nh√≥m ƒëi·ªÅu tr·ªã")["Tr·ªã gi√°"].sum().reset_index().sort_values("Tr·ªã gi√°", ascending=False)
            fig_w2 = px.pie(treat_win, names="Nh√≥m ƒëi·ªÅu tr·ªã", values="Tr·ªã gi√°", title="C∆° c·∫•u tr·ªã gi√° theo Nh√≥m ƒëi·ªÅu tr·ªã (Tr√∫ng th·∫ßu)")
            st.plotly_chart(fig_w2, use_container_width=True)
            # N·∫øu c√≥ upload danh m·ª•c m·ªùi th·∫ßu ƒë·ªÉ ƒë·ªëi chi·∫øu
            if invite_file is not None:
                xls_inv = pd.ExcelFile(invite_file)
                inv_sheet = xls_inv.sheet_names[0]
                df_inv_raw = pd.read_excel(invite_file, sheet_name=inv_sheet, header=None)
                header_idx2 = None
                for i in range(10):
                    row_text = " ".join(df_inv_raw.iloc[i].fillna('').astype(str).tolist())
                    if "T√™n ho·∫°t ch·∫•t" in row_text and "S·ªë l∆∞·ª£ng" in row_text:
                        header_idx2 = i
                        break
                if header_idx2 is not None:
                    header2 = df_inv_raw.iloc[header_idx2].tolist()
                    df_inv_full = df_inv_raw.iloc[header_idx2+1:].reset_index(drop=True)
                    df_inv_full.columns = header2
                    df_inv_full = df_inv_full.dropna(how='all').reset_index(drop=True)
                    # So s√°nh c√°c m·ª•c kh√¥ng tr√∫ng (c√≥ trong m·ªùi th·∫ßu nh∆∞ng kh√¥ng c√≥ trong tr√∫ng th·∫ßu)
                    if "M√£ ph·∫ßn (L√¥)" in df_inv_full.columns and "M√£ ph·∫ßn (L√¥)" in df_win.columns:
                        inv_ids = set(df_inv_full["M√£ ph·∫ßn (L√¥)"].astype(str))
                        win_ids = set(df_win["M√£ ph·∫ßn (L√¥)"].astype(str))
                        missing_ids = inv_ids - win_ids
                        missing_items = df_inv_full[df_inv_full["M√£ ph·∫ßn (L√¥)"].astype(str).isin(missing_ids)]
                    else:
                        # D√πng k·∫øt h·ª£p ho·∫°t ch·∫•t + h√†m l∆∞·ª£ng ƒë·ªÉ ƒë·ªëi chi·∫øu n·∫øu kh√¥ng c√≥ M√£ ph·∫ßn
                        inv_keys = df_inv_full["T√™n ho·∫°t ch·∫•t"].astype(str) + df_inv_full["N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng"].astype(str)
                        win_keys = df_win["T√™n ho·∫°t ch·∫•t"].astype(str) + df_win["N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng"].astype(str)
                        missing_mask = ~inv_keys.isin(win_keys)
                        missing_items = df_inv_full[missing_mask]
                    if not missing_items.empty:
                        st.write("**C√°c thu·ªëc m·ªùi th·∫ßu kh√¥ng c√≥ nh√† th·∫ßu tr√∫ng:**")
                        st.dataframe(missing_items[["T√™n ho·∫°t ch·∫•t", "N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng", "S·ªë l∆∞·ª£ng", "Gi√° k·∫ø ho·∫°ch"]])
                        st.write(f"üìå S·ªë l∆∞·ª£ng thu·ªëc kh√¥ng tr√∫ng th·∫ßu: {len(missing_items)}")
                        # L∆∞u v√†o session_state ƒë·ªÉ d√πng cho ƒë·ªÅ xu·∫•t
                        st.session_state["missing_items"] = missing_items
                    else:
                        st.write("‚úÖ T·∫•t c·∫£ thu·ªëc m·ªùi th·∫ßu ƒë·ªÅu ƒë√£ c√≥ nh√† th·∫ßu tr√∫ng.")

# 4. ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai
elif option == "ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai":
    st.header("üí° ƒê·ªÅ Xu·∫•t H∆∞·ªõng Tri·ªÉn Khai")
    if "filtered_df" not in st.session_state:
        st.info("Vui l√≤ng th·ª±c hi·ªán ph√¢n t√≠ch tr∆∞·ªõc ƒë·ªÉ c√≥ d·ªØ li·ªáu.")
    else:
        df_filtered = st.session_state["filtered_df"]
        hospital = st.session_state.get("selected_hospital", "")
        # Danh s√°ch ƒë·ªÅ xu·∫•t
        suggestions_yes = []  # n√™n tri·ªÉn khai
        suggestions_no = []   # kh√¥ng n√™n tri·ªÉn khai
        # 1. C√°c s·∫£n ph·∫©m trong danh m·ª•c c√¥ng ty t·∫°i b·ªánh vi·ªán nh∆∞ng ch∆∞a c√≥ trong danh m·ª•c m·ªùi th·∫ßu
        hosp_products = set(file3[file3["B·ªánh vi·ªán/SYT"] == hospital]["T√™n s·∫£n ph·∫©m"])
        included_products = set(df_filtered["T√™n s·∫£n ph·∫©m"])
        not_included = hosp_products - included_products
        # X√°c ƒë·ªãnh nh√≥m b·ªánh vi·ªán t∆∞∆°ng t·ª± (c√πng Mi·ªÅn, c√πng lo·∫°i SYT ho·∫∑c BV)
        hosp_info = file3[file3["B·ªánh vi·ªán/SYT"] == hospital].iloc[0] if not file3[file3["B·ªánh vi·ªán/SYT"] == hospital].empty else None
        similar_df = file3.copy()
        if hosp_info is not None:
            if "SYT" in hospital:
                # c√°c S·ªü Y T·∫ø kh√°c trong c√πng Mi·ªÅn
                similar_df = similar_df[similar_df["B·ªánh vi·ªán/SYT"].str.contains("SYT") & (similar_df["Mi·ªÅn"] == hosp_info["Mi·ªÅn"])]
            else:
                # c√°c B·ªánh vi·ªán kh√°c (kh√¥ng ph·∫£i SYT) trong c√πng Mi·ªÅn
                similar_df = similar_df[~similar_df["B·ªánh vi·ªán/SYT"].str.contains("SYT") & (similar_df["Mi·ªÅn"] == hosp_info["Mi·ªÅn"])]
        for prod in not_included:
            if prod in set(similar_df["T√™n s·∫£n ph·∫©m"]):
                suggestions_yes.append(f"- N√™n tri·ªÉn khai **{prod}**: S·∫£n ph·∫©m ch∆∞a c√≥ trong th·∫ßu c·ªßa {hospital}, nh∆∞ng nhi·ªÅu ƒë∆°n v·ªã t∆∞∆°ng t·ª± ƒë√£ c√≥ nhu c·∫ßu.")
            else:
                suggestions_no.append(f"- Ch∆∞a c·∫ßn tri·ªÉn khai **{prod}**: S·∫£n ph·∫©m ch∆∞a c√≥ trong th·∫ßu {hospital} v√† ch∆∞a ph·ªï bi·∫øn ·ªü nh√≥m b·ªánh vi·ªán t∆∞∆°ng t·ª±.")
        # 2. C√°c s·∫£n ph·∫©m m·ªùi th·∫ßu nh∆∞ng kh√¥ng c√≥ nh√† th·∫ßu tr√∫ng (n·∫øu c√≥)
        if "missing_items" in st.session_state:
            missing_items = st.session_state["missing_items"]
            for _, row in missing_items.iterrows():
                suggestions_yes.append(f"- Th·ª≠ tri·ªÉn khai **{row['T√™n ho·∫°t ch·∫•t']}**: Thu·ªëc ƒë∆∞·ª£c m·ªùi th·∫ßu {hospital} nh∆∞ng ch∆∞a c√≥ nh√† th·∫ßu tr√∫ng, c√≥ th·ªÉ l√† c∆° h·ªôi ƒë∆∞a s·∫£n ph·∫©m v√†o.")
        # 3. C√°c s·∫£n ph·∫©m c√≥ ƒë·ªëi th·ªß tr√∫ng th·∫ßu (c√¥ng ty ch∆∞a tr√∫ng)
        # Gi·∫£ s·ª≠ c√¥ng ty theo d√µi c√°c s·∫£n ph·∫©m ƒë√£ ƒë∆∞·ª£c ƒë∆∞a v√†o th·∫ßu (df_filtered), n·∫øu kh√¥ng tr√∫ng th·∫ßu c√≥ th·ªÉ c√¢n nh·∫Øc m·ª©c ƒë·ªô ∆∞u ti√™n
        if "missing_items" in st.session_state or "filtered_df" in st.session_state:
            # N·∫øu m·ªôt s·∫£n ph·∫©m c√≥ m·∫∑t trong danh m·ª•c m·ªùi th·∫ßu (c·ªßa c√¥ng ty) nh∆∞ng c√¥ng ty kh√¥ng tr√∫ng -> ƒë·ªëi th·ªß ƒë√£ tr√∫ng
            # (ƒê∆°n gi·∫£n coi nh∆∞ m·ªçi m·ª•c trong df_filtered l√† c√¥ng ty c√≥ tham gia, n·∫øu kh√¥ng n·∫±m trong missing_items t·ª©c l√† c√≥ ng∆∞·ªùi tr√∫ng)
            if "missing_items" in st.session_state:
                lost_df = df_filtered.copy()
                for _, miss in st.session_state["missing_items"].iterrows():
                    # lo·∫°i c√°c m·ª•c kh√¥ng ai tr√∫ng (ƒë√£ x·ª≠ l√Ω ·ªü tr√™n)
                    lost_df = lost_df[~((lost_df["T√™n ho·∫°t ch·∫•t"] == miss["T√™n ho·∫°t ch·∫•t"]) & (lost_df["N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng"] == miss["N·ªìng ƒë·ªô/h√†m l∆∞·ª£ng"]))]
            else:
                lost_df = df_filtered
            # T·∫•t c·∫£ m·ª•c c√≤n l·∫°i trong lost_df coi nh∆∞ c√≥ ƒë·ªëi th·ªß tr√∫ng
            for _, row in lost_df.iterrows():
                suggestions_no.append(f"- H·∫°n ch·∫ø t·∫≠p trung **{row['T√™n ho·∫°t ch·∫•t']}**: ƒê√£ c√≥ ƒë·ªëi th·ªß tr√∫ng th·∫ßu t·∫°i {hospital}, c·∫ßn c√¢n nh·∫Øc n·∫øu kh√¥ng c√≥ l·ª£i th·∫ø c·∫°nh tranh.")
        # Hi·ªÉn th·ªã ƒë·ªÅ xu·∫•t
        st.subheader("üî∏ ƒê·ªÅ xu·∫•t n√™n tri·ªÉn khai")
        if suggestions_yes:
            st.markdown("\n".join(suggestions_yes))
        else:
            st.write("Kh√¥ng c√≥ s·∫£n ph·∫©m m·ªõi n√†o c·∫ßn tri·ªÉn khai th√™m t·∫°i th·ªùi ƒëi·ªÉm n√†y.")
        st.subheader("üîπ ƒê·ªÅ xu·∫•t kh√¥ng n√™n tri·ªÉn khai")
        if suggestions_no:
            st.markdown("\n".join(suggestions_no))
        else:
            st.write("Kh√¥ng c√≥ s·∫£n ph·∫©m n√†o c·∫ßn ng·ª´ng tri·ªÉn khai; ti·∫øp t·ª•c duy tr√¨ c√°c danh m·ª•c hi·ªán c√≥.")
